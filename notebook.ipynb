{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from itertools import islice\n",
    "from pathlib import Path\n",
    "from shutil import copyfileobj\n",
    "from typing import Iterator, Optional, Union\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "from requests import Session\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import SoupStrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApacheNode:\n",
    "    \"\"\"\n",
    "    This represents a tree node in the Apache directory page for the NOAA NOMADS site;\n",
    "    for more details see https://nomads.ncep.noaa.gov/\n",
    "\n",
    "    It's used to look up TSRAGR data. TSRAGR is Terminal Aerodrome Forecast (TAF) code\n",
    "    for a thunderstorm with hail.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, session: Session, url: str, name: Optional[str] = None) -> None:\n",
    "        self.session = session\n",
    "        self.url = url\n",
    "\n",
    "        if name is None:\n",
    "            name = url.removesuffix(\"/\").rsplit(\"/\", maxsplit=1)[-1]\n",
    "        self.name = name\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.url\n",
    "\n",
    "\n",
    "class ApacheFile(ApacheNode):\n",
    "    def download(self, save_to: Path) -> None:\n",
    "        print(f\"Downloading {self.name}...\", end=\" \")\n",
    "        local_filename = save_to / self.name\n",
    "\n",
    "        with self.session.get(self.url, stream=True) as response:\n",
    "            response.raise_for_status()\n",
    "            with local_filename.open(\"wb\") as f:\n",
    "                copyfileobj(response.raw, f)\n",
    "\n",
    "        print(\"saved\")\n",
    "\n",
    "\n",
    "class ApacheDir(ApacheNode):\n",
    "    pre_strainer = SoupStrainer(name=\"pre\")\n",
    "\n",
    "    # Text has at least one character and cannot contain Parent Directory\n",
    "    link_pattern = re.compile(\n",
    "        \"(?i)\"  # ignore case\n",
    "        \"^\"  # string start\n",
    "        \"(?:\"  # non-capturing group\n",
    "        \"(?!parent directory)\"  # negative lookahead: don't match 'parent directory'\n",
    "        \".\"  # match any one character\n",
    "        \")+\"  # match one or more of the above chars\n",
    "        \"$\"  # string end\n",
    "    )\n",
    "\n",
    "    def __init__(self, session: Session, url: str, name: Optional[str] = None) -> None:\n",
    "        if not url.endswith(\"/\"):\n",
    "            url += \"/\"\n",
    "        super().__init__(session, url, name)\n",
    "\n",
    "    def children(self) -> Iterator[ApacheNode]:\n",
    "        with self.session.get(self.url) as response:\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(\n",
    "                markup=response.text, features=\"lxml\", parse_only=self.pre_strainer\n",
    "            )\n",
    "        pre = soup.pre\n",
    "        anchors = pre.find_all(name=\"a\", text=self.link_pattern, recursive=False)\n",
    "\n",
    "        for anchor in anchors:\n",
    "            child_name = anchor[\"href\"]\n",
    "            child_url = urljoin(self.url, child_name)\n",
    "            size_text = anchor.next_sibling.strip()\n",
    "            if size_text.endswith(\"-\"):\n",
    "                child_type = ApacheDir\n",
    "            else:\n",
    "                child_type = ApacheFile\n",
    "            yield child_type(self.session, child_url, child_name)\n",
    "\n",
    "    def navto(self, *args: str) -> \"ApacheDir\":\n",
    "        url = urljoin(self.url, \"/\".join(args))\n",
    "        return ApacheDir(self.session, url=url, name=args[-1])\n",
    "\n",
    "    def inav(self, index: int) -> \"ApacheNode\":\n",
    "        (child,) = islice(self.children(), index, index + 1)\n",
    "        return child\n",
    "\n",
    "    def iterfiles(self):\n",
    "        for node in self.children():\n",
    "            if isinstance(node, ApacheFile):\n",
    "                yield node\n",
    "\n",
    "    def iterdir(self):\n",
    "        for node in self.children():\n",
    "            if isinstance(node, ApacheDir):\n",
    "                yield node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class NotAvaliable(Exception):\n",
    "    ...\n",
    "\n",
    "\n",
    "def daily_download(\n",
    "    base_url: str,\n",
    "    path_opts: str,\n",
    "    target_day=int,  # datetime.datetime,\n",
    ") -> None:\n",
    "\n",
    "    with Session() as session:\n",
    "        ragr = ApacheDir(session, url=base_url)\n",
    "\n",
    "        for path in ragr.navto(path_opts).iterdir():\n",
    "            date = datetime.datetime.strptime(\n",
    "                re.search(r\"\\d+/$\", path.url).group(), \"%Y%m%d/\"\n",
    "            )\n",
    "\n",
    "            if date.day == target_day:\n",
    "                if \"hrrr\" in path.url:\n",
    "                    path = path.navto(\"conus\")\n",
    "\n",
    "                for file in path.iterfiles():\n",
    "                    url = file.url\n",
    "                    filename = url.rsplit(\"/\", maxsplit=1)[-1]\n",
    "                    time_delta = datetime.timedelta(hours=int(url[-4:]))\n",
    "                    valid_time = date + time_delta\n",
    "                    # save_to = Path('/media/external/data/')  # '/media/external/data/'\n",
    "                    # save_to.mkdir(exist_ok=True)\n",
    "                    if valid_time.day == target_day:\n",
    "                        ...\n",
    "                        # file.download()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    daily_download(\n",
    "        \"https://nomads.ncep.noaa.gov/pub/data\",\n",
    "        \"nccf/com/557ww/prod\",\n",
    "        target_day=datetime.datetime.utcnow().day,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('wxlab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee8bdae2639ac3626157e8ee290642a9bef975c9dba8b8afc6858052d7b5d627"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
